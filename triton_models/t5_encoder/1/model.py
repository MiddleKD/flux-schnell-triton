#!/usr/bin/env python3
"""
T5 Text Encoder for Triton Inference Server

FLUX pipelineì˜ _get_t5_prompt_embeds ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ë¥¼ T5 sequence embeddingsë¡œ ë³€í™˜í•˜ëŠ” GPU ëª¨ë¸ì…ë‹ˆë‹¤.

Based on flux_pipeline.py lines 218-265
"""

import os
import sys
import json
import numpy as np
from typing import Dict, List, Optional

# Triton Python Backend
try:
    import triton_python_backend_utils as pb_utils
    TRITON_AVAILABLE = True
except ImportError:
    TRITON_AVAILABLE = False
    print("âš ï¸ Triton Python Backend not available - running in test mode")

# DLPack ì§€ì› í™•ì¸ (í—Œì¥ ìš”êµ¬ì‚¬í•­: ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜)
try:
    import torch.utils.dlpack
    DLPACK_AVAILABLE = True
except ImportError:
    DLPACK_AVAILABLE = False

# í•„ìˆ˜ ì˜ì¡´ì„±
try:
    import torch
    from transformers import T5EncoderModel, T5TokenizerFast
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½: {e}")
    sys.exit(1)


class TritonPythonModel:
    """T5 Text Encoder Triton Model"""

    def initialize(self, args: Dict) -> None:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”„ T5 Encoder ì´ˆê¸°í™” ì¤‘...")

        # ëª¨ë¸ ì„¤ì • ë¡œë“œ
        self.model_config = json.loads(args['model_config'])

        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.model_name = self._get_parameter("model_name", "black-forest-labs/FLUX.1-schnell")
        self.max_length = int(self._get_parameter("max_sequence_length", "512"))
        self.embedding_dim = int(self._get_parameter("embedding_dim", "4096"))

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # T5 ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ T5 ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        self.tokenizer = T5TokenizerFast.from_pretrained(
            self.model_name,
            subfolder="tokenizer_2"
        )
        self.text_encoder = T5EncoderModel.from_pretrained(
            self.model_name,
            subfolder="text_encoder_2"
        )
        self.text_encoder.to(self.device)
        self.text_encoder.eval()
        print(f"âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")

    def _get_parameter(self, key: str, default: str = "") -> str:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        for param in self.model_config.get('parameters', []):
            if param['key'] == key:
                return param['value']['string_value']
        return default

    def execute(self, requests: List) -> List:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        responses = []

        for request in requests:
            try:
                response = self._process_request(request)
                responses.append(response)
            except Exception as e:
                error_msg = f"T5 Encoder ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                print(f"âŒ {error_msg}")

                # ì—ëŸ¬ ì‘ë‹µ ìƒì„±
                error_response = pb_utils.InferenceResponse(
                    error=pb_utils.TritonError(error_msg)
                ) if TRITON_AVAILABLE else None
                responses.append(error_response)

        return responses

    def _process_request(self, request):
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
        # ì…ë ¥ í…ì„œ ì¶”ì¶œ
        input_ids_tensor = pb_utils.get_input_tensor_by_name(request, "input_ids") if TRITON_AVAILABLE else None
        attention_mask_tensor = pb_utils.get_input_tensor_by_name(request, "attention_mask") if TRITON_AVAILABLE else None

        if not TRITON_AVAILABLE:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë”ë¯¸ ë°ì´í„°
            batch_size = 1
            input_ids = torch.randint(0, 32127, (batch_size, self.max_length), dtype=torch.long)
            attention_mask = torch.ones((batch_size, self.max_length), dtype=torch.long)
        else:
            # Triton ëª¨ë“œ: ì‹¤ì œ ë°ì´í„°
            input_ids = self._tensor_to_torch(input_ids_tensor)
            attention_mask = self._tensor_to_torch(attention_mask_tensor) if attention_mask_tensor else None

        # T5 ì¸ì½”ë”© ìˆ˜í–‰ (flux_pipeline.py ë¡œì§ ê¸°ë°˜)
        sequence_embeds, text_ids = self._encode_text(input_ids, attention_mask)

        # ì‘ë‹µ ìƒì„±
        if TRITON_AVAILABLE:
            output_tensors = [
                self._torch_to_tensor(sequence_embeds, "sequence_embeds"),
                self._torch_to_tensor(text_ids, "text_ids")
            ]
            return pb_utils.InferenceResponse(output_tensors)
        else:
            return {"sequence_embeds": sequence_embeds, "text_ids": text_ids}

    def _tensor_to_torch(self, triton_tensor) -> torch.Tensor:
        """Triton í…ì„œë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE and hasattr(triton_tensor, 'to_dlpack'):
            # DLPack ì‚¬ìš© (ë©”ëª¨ë¦¬ ë³µì‚¬ ìµœì†Œí™”)
            return torch.utils.dlpack.from_dlpack(triton_tensor.to_dlpack())
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = triton_tensor.as_numpy()
            return torch.from_numpy(numpy_array).to(self.device)

    def _torch_to_tensor(self, torch_tensor: torch.Tensor, name: str):
        """PyTorch í…ì„œë¥¼ Triton í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE:
            # DLPack ì‚¬ìš©
            return pb_utils.Tensor.from_dlpack(name, torch.utils.dlpack.to_dlpack(torch_tensor))
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = torch_tensor.detach().cpu().numpy()
            return pb_utils.Tensor(name, numpy_array)

    def _encode_text(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]) -> tuple:
        """
        T5 í…ìŠ¤íŠ¸ ì¸ì½”ë”©

        flux_pipeline.pyì˜ _get_t5_prompt_embeds ë©”ì„œë“œ ê¸°ë°˜ (lines 218-265)
        """
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)

        batch_size = input_ids.shape[0]

        with torch.no_grad():
            # T5 í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‹¤í–‰ (flux_pipeline.py line 254)
            encoder_output = self.text_encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=False
            )

            # Last hidden state ì‚¬ìš©
            sequence_embeds = encoder_output.last_hidden_state

            # ë°ì´í„° íƒ€ì… í™•ì¸
            dtype = self.text_encoder.dtype
            sequence_embeds = sequence_embeds.to(dtype=dtype, device=self.device)

            # sequence length í™•ì¸ (flux_pipeline.py line 259)
            _, seq_len, _ = sequence_embeds.shape

            # text_ids ìƒì„± (flux_pipeline.py line 386)
            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ IDëŠ” ëª¨ë“  ìš”ì†Œê°€ 0ì¸ í…ì„œ
            text_ids = torch.zeros(seq_len, 3, dtype=dtype, device=self.device)

            print(f"ğŸ“Š T5 sequence embeds í˜•íƒœ: {sequence_embeds.shape}")  # [batch_size, seq_len, 4096]
            print(f"ğŸ“Š T5 text_ids í˜•íƒœ: {text_ids.shape}")  # [seq_len, 3]

            return sequence_embeds, text_ids

    def finalize(self) -> None:
        """ëª¨ë¸ ì •ë¦¬"""
        print("ğŸ”„ T5 Encoder ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'text_encoder'):
            del self.text_encoder
        if hasattr(self, 'tokenizer'):
            del self.tokenizer

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("âœ… T5 Encoder ì •ë¦¬ ì™„ë£Œ")


def test_t5_encoder():
    """ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (í—Œì¥ ìš”êµ¬ì‚¬í•­)"""
    print("ğŸš€ T5 Encoder í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
    os.environ["TEST_MODE"] = "true"

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TritonPythonModel()
    args = {
        'model_config': json.dumps({
            'parameters': [
                {'key': 'model_name', 'value': {'string_value': 'black-forest-labs/FLUX.1-schnell'}},
                {'key': 'max_sequence_length', 'value': {'string_value': '512'}},
                {'key': 'embedding_dim', 'value': {'string_value': '4096'}}
            ]
        })
    }

    try:
        model.initialize(args)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        batch_size = 2
        max_length = 512
        input_ids = torch.randint(0, 32127, (batch_size, max_length), dtype=torch.long)
        attention_mask = torch.ones((batch_size, max_length), dtype=torch.long)

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì…ë ¥ í˜•íƒœ: {input_ids.shape}")

        # ì¶”ë¡  ì‹¤í–‰
        sequence_embeds, text_ids = model._encode_text(input_ids, attention_mask)

        # ê²°ê³¼ ê²€ì¦
        expected_seq_shape = (batch_size, max_length, 4096)
        expected_text_ids_shape = (max_length, 3)

        if sequence_embeds.shape == expected_seq_shape:
            print(f"âœ… sequence_embeds í˜•íƒœ ê²€ì¦ í†µê³¼: {sequence_embeds.shape}")
        else:
            print(f"âŒ sequence_embeds í˜•íƒœ ì˜¤ë¥˜: {sequence_embeds.shape}, ì˜ˆìƒ: {expected_seq_shape}")
            return False

        if text_ids.shape == expected_text_ids_shape:
            print(f"âœ… text_ids í˜•íƒœ ê²€ì¦ í†µê³¼: {text_ids.shape}")
        else:
            print(f"âŒ text_ids í˜•íƒœ ì˜¤ë¥˜: {text_ids.shape}, ì˜ˆìƒ: {expected_text_ids_shape}")
            return False

        # ê°’ ë²”ìœ„ ê²€ì¦
        min_val, max_val = sequence_embeds.min().item(), sequence_embeds.max().item()
        print(f"ğŸ“Š sequence_embeds ê°’ ë²”ìœ„: {min_val:.4f} ~ {max_val:.4f}")

        text_ids_sum = text_ids.sum().item()
        print(f"ğŸ“Š text_ids í•©ê³„: {text_ids_sum} (0ì´ì–´ì•¼ í•¨)")

        model.finalize()
        print("ğŸ‰ T5 Encoder í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    success = test_t5_encoder()
    sys.exit(0 if success else 1)