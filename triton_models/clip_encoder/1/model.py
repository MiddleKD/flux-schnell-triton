#!/usr/bin/env python3
"""
CLIP Text Encoder for Triton Inference Server

FLUX pipelineì˜ _get_clip_prompt_embeds ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ë¥¼ CLIP pooled embeddingsë¡œ ë³€í™˜í•˜ëŠ” GPU ëª¨ë¸ì…ë‹ˆë‹¤.

Based on flux_pipeline.py lines 267-309
"""

import os
import sys
import json
import numpy as np
import logging
from typing import Dict, List, Optional

# ===================================================================
# ê³µí†µ í•¨ìˆ˜ë“¤ (common_functions_template.pyì—ì„œ ë³µì‚¬)
# ===================================================================

def check_triton_availability():
    """Triton Python Backend ê°€ìš©ì„± ì²´í¬ ë° Mock ì„¤ì •"""
    try:
        import triton_python_backend_utils as pb_utils
        return True, pb_utils
    except ImportError:
        logging.warning("Triton backend not available - running in test mode")

        # Mock pb_utils for testing
        class MockTensor:
            def __init__(self, name, data):
                self.name = name
                self.data = data

        class MockInferenceRequest:
            def __init__(self):
                pass

        class MockInferenceResponse:
            def __init__(self, output_tensors=None, error=None):
                self.output_tensors = output_tensors or []
                self.error_msg = error

        class MockTritonError:
            def __init__(self, message):
                self.message = message

        class MockPbUtils:
            Tensor = MockTensor
            InferenceRequest = MockInferenceRequest
            InferenceResponse = MockInferenceResponse
            TritonError = MockTritonError

            @staticmethod
            def get_input_tensor_by_name(request, name):
                return None

            @staticmethod
            def get_output_tensor_by_name(response, name):
                return None

        return False, MockPbUtils()

def check_dlpack_availability():
    """DLPack ê°€ìš©ì„± ì²´í¬"""
    try:
        import torch.utils.dlpack
        from torch.utils.dlpack import to_dlpack, from_dlpack
        return True, to_dlpack, from_dlpack
    except ImportError:
        logging.warning("DLPack not available - falling back to standard tensor operations")
        return False, None, None

def check_model_dependencies(required_modules: List[str]) -> bool:
    """í•„ìˆ˜ ì˜ì¡´ì„± ì²´í¬"""
    missing_modules = []
    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing_modules.append(module)

    if missing_modules:
        logging.error(f"Missing required modules: {missing_modules}")
        return False
    return True

def extract_model_parameters(model_config: Dict) -> Dict[str, str]:
    """ëª¨ë¸ configì—ì„œ ëª¨ë“  íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    params = {}
    for param in model_config.get('parameters', []):
        params[param['key']] = param['value']['string_value']
    return params

def setup_logging(model_name: str):
    """ëª¨ë¸ë³„ ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format=f'[{model_name}] %(asctime)s - %(levelname)s - %(message)s'
    )

def create_error_response(pb_utils, error_message: str, triton_available: bool):
    """í‘œì¤€í™”ëœ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
    if triton_available:
        return pb_utils.InferenceResponse(
            output_tensors=[],
            error=pb_utils.TritonError(error_message)
        )
    else:
        # Test modeì—ì„œëŠ” None ë°˜í™˜
        logging.error(f"Error (test mode): {error_message}")
        return None

def handle_model_error(pb_utils, triton_available: bool, error: Exception, context: str = ""):
    """ëª¨ë¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
    error_msg = f"{context} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(error)}"
    logging.error(error_msg)
    return create_error_response(pb_utils, error_msg, triton_available)

def setup_device(prefer_cuda: bool = True):
    """ìµœì  ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    try:
        import torch
        if prefer_cuda and torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Using CUDA device: {torch.cuda.get_device_name()}")
        else:
            device = torch.device("cpu")
            logging.info("Using CPU device")
        return device
    except ImportError:
        logging.warning("PyTorch not available, device setup skipped")
        return None

def initialize_model_base(args: Dict, model_name: str, required_modules: List[str]):
    """ëª¨ë“  ëª¨ë¸ì˜ ê³µí†µ ì´ˆê¸°í™” ë¡œì§"""
    # ë¡œê¹… ì„¤ì •
    setup_logging(model_name)
    logging.info(f"{model_name} ì´ˆê¸°í™” ì‹œì‘...")

    # ì˜ì¡´ì„± ì²´í¬
    if not check_model_dependencies(required_modules):
        logging.error(f"{model_name} ì˜ì¡´ì„± ì²´í¬ ì‹¤íŒ¨")
        sys.exit(1)

    # Triton & DLPack ê°€ìš©ì„± ì²´í¬
    triton_available, pb_utils = check_triton_availability()
    dlpack_available, to_dlpack, from_dlpack = check_dlpack_availability()

    # ëª¨ë¸ config íŒŒì‹±
    model_config = json.loads(args['model_config'])
    params = extract_model_parameters(model_config)

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = setup_device()

    return {
        'triton_available': triton_available,
        'pb_utils': pb_utils,
        'dlpack_available': dlpack_available,
        'to_dlpack': to_dlpack,
        'from_dlpack': from_dlpack,
        'model_config': model_config,
        'params': params,
        'device': device
    }

# ===================================================================

# í•„ìˆ˜ ì˜ì¡´ì„±
try:
    import torch
    from transformers import CLIPTextModel, CLIPTokenizer
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½: {e}")
    sys.exit(1)


class TritonPythonModel:
    """CLIP Text Encoder Triton Model"""

    def initialize(self, args: Dict) -> None:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        # ê³µí†µ ì´ˆê¸°í™” ë¡œì§ ì‚¬ìš©
        init_result = initialize_model_base(
            args,
            "CLIP_Encoder",
            ["torch", "transformers"]  # í•„ìˆ˜ ì˜ì¡´ì„±
        )

        # ì´ˆê¸°í™” ê²°ê³¼ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
        self.triton_available = init_result['triton_available']
        self.pb_utils = init_result['pb_utils']
        self.dlpack_available = init_result['dlpack_available']
        self.to_dlpack = init_result['to_dlpack']
        self.from_dlpack = init_result['from_dlpack']
        self.model_config = init_result['model_config']
        self.device = init_result['device']
        params = init_result['params']

        # CLIP íŠ¹í™” íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.model_name = params.get("model_name", "black-forest-labs/FLUX.1-schnell")
        self.max_length = int(params.get("max_sequence_length", "77"))
        self.embedding_dim = int(params.get("embedding_dim", "768"))

        # CLIP ëª¨ë¸ ë¡œë“œ
        logging.info(f"CLIP ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        self.tokenizer = CLIPTokenizer.from_pretrained(
            self.model_name,
            subfolder="tokenizer"
        )
        self.text_encoder = CLIPTextModel.from_pretrained(
            self.model_name,
            subfolder="text_encoder"
        )
        self.text_encoder.to(self.device)
        self.text_encoder.eval()
        logging.info(f"CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")

    def execute(self, requests: List) -> List:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        responses = []

        for request in requests:
            try:
                response = self._process_request(request)
                responses.append(response)
            except Exception as e:
                error_response = handle_model_error(
                    self.pb_utils,
                    self.triton_available,
                    e,
                    "CLIP Encoder"
                )
                responses.append(error_response)

        return responses

    def _process_request(self, request):
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
        # ì…ë ¥ í…ì„œ ì¶”ì¶œ
        input_ids_tensor = pb_utils.get_input_tensor_by_name(request, "input_ids") if TRITON_AVAILABLE else None
        attention_mask_tensor = pb_utils.get_input_tensor_by_name(request, "attention_mask") if TRITON_AVAILABLE else None

        if not TRITON_AVAILABLE:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë”ë¯¸ ë°ì´í„°
            batch_size = 1
            input_ids = torch.randint(0, 49407, (batch_size, self.max_length), dtype=torch.long)
            attention_mask = torch.ones((batch_size, self.max_length), dtype=torch.long)
        else:
            # Triton ëª¨ë“œ: ì‹¤ì œ ë°ì´í„°
            input_ids = self._tensor_to_torch(input_ids_tensor)
            attention_mask = self._tensor_to_torch(attention_mask_tensor) if attention_mask_tensor else None

        # CLIP ì¸ì½”ë”© ìˆ˜í–‰ (flux_pipeline.py ë¡œì§ ê¸°ë°˜)
        pooled_embeds = self._encode_text(input_ids, attention_mask)

        # ì‘ë‹µ ìƒì„±
        if TRITON_AVAILABLE:
            output_tensor = self._torch_to_tensor(pooled_embeds, "pooled_embeds")
            return pb_utils.InferenceResponse([output_tensor])
        else:
            return pooled_embeds

    def _tensor_to_torch(self, triton_tensor) -> torch.Tensor:
        """Triton í…ì„œë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE and hasattr(triton_tensor, 'to_dlpack'):
            # DLPack ì‚¬ìš© (ë©”ëª¨ë¦¬ ë³µì‚¬ ìµœì†Œí™”)
            return torch.utils.dlpack.from_dlpack(triton_tensor.to_dlpack())
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = triton_tensor.as_numpy()
            return torch.from_numpy(numpy_array).to(self.device)

    def _torch_to_tensor(self, torch_tensor: torch.Tensor, name: str):
        """PyTorch í…ì„œë¥¼ Triton í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE:
            # DLPack ì‚¬ìš©
            return pb_utils.Tensor.from_dlpack(name, torch.utils.dlpack.to_dlpack(torch_tensor))
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = torch_tensor.detach().cpu().numpy()
            return pb_utils.Tensor(name, numpy_array)

    def _encode_text(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """
        CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”©

        flux_pipeline.pyì˜ _get_clip_prompt_embeds ë©”ì„œë“œ ê¸°ë°˜ (lines 267-309)
        """
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)

        with torch.no_grad():
            # CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‹¤í–‰
            text_encoder_output = self.text_encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=False
            )

            # Pooled output ì‚¬ìš© (flux_pipeline.py line 302)
            pooled_embeds = text_encoder_output.pooler_output

            # ë°ì´í„° íƒ€ì… í™•ì¸
            pooled_embeds = pooled_embeds.to(dtype=self.text_encoder.dtype, device=self.device)

            print(f"ğŸ“Š CLIP ì¶œë ¥ í˜•íƒœ: {pooled_embeds.shape}")  # [batch_size, 768]

            return pooled_embeds

    def finalize(self) -> None:
        """ëª¨ë¸ ì •ë¦¬"""
        print("ğŸ”„ CLIP Encoder ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'text_encoder'):
            del self.text_encoder
        if hasattr(self, 'tokenizer'):
            del self.tokenizer

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("âœ… CLIP Encoder ì •ë¦¬ ì™„ë£Œ")


def test_clip_encoder():
    """ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (í—Œì¥ ìš”êµ¬ì‚¬í•­)"""
    print("ğŸš€ CLIP Encoder í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
    os.environ["TEST_MODE"] = "true"

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TritonPythonModel()
    args = {
        'model_config': json.dumps({
            'parameters': [
                {'key': 'model_name', 'value': {'string_value': 'black-forest-labs/FLUX.1-schnell'}},
                {'key': 'max_sequence_length', 'value': {'string_value': '77'}},
                {'key': 'embedding_dim', 'value': {'string_value': '768'}}
            ]
        })
    }

    try:
        model.initialize(args)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        batch_size = 2
        input_ids = torch.randint(0, 49407, (batch_size, 77), dtype=torch.long)
        attention_mask = torch.ones((batch_size, 77), dtype=torch.long)

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì…ë ¥ í˜•íƒœ: {input_ids.shape}")

        # ì¶”ë¡  ì‹¤í–‰
        pooled_embeds = model._encode_text(input_ids, attention_mask)

        # ê²°ê³¼ ê²€ì¦
        expected_shape = (batch_size, 768)
        if pooled_embeds.shape == expected_shape:
            print(f"âœ… ì¶œë ¥ í˜•íƒœ ê²€ì¦ í†µê³¼: {pooled_embeds.shape}")
        else:
            print(f"âŒ ì¶œë ¥ í˜•íƒœ ì˜¤ë¥˜: {pooled_embeds.shape}, ì˜ˆìƒ: {expected_shape}")
            return False

        # ê°’ ë²”ìœ„ ê²€ì¦
        min_val, max_val = pooled_embeds.min().item(), pooled_embeds.max().item()
        print(f"ğŸ“Š ì¶œë ¥ ê°’ ë²”ìœ„: {min_val:.4f} ~ {max_val:.4f}")

        model.finalize()
        print("ğŸ‰ CLIP Encoder í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    success = test_clip_encoder()
    sys.exit(0 if success else 1)