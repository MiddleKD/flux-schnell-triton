# CLIP Text Encoder Configuration
# CLIP 모델을 사용한 텍스트 인코딩 서비스
# GPU에서 실행되며 텍스트를 pooled embeddings로 변환

name: "clip_encoder"
platform: "python"
max_batch_size: 16

input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ 77 ]  # CLIP max sequence length
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT32
    dims: [ 77 ]
    optional: true
  }
]

output [
  {
    name: "pooled_embeds"
    data_type: TYPE_FP32
    dims: [ 768 ]  # CLIP embedding dimension
  }
]

# GPU 인스턴스 설정
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

# 동적 배치 설정
dynamic_batching {
  max_queue_delay_microseconds: 50000  # 50ms
  preferred_batch_size: [ 4, 8, 16 ]
}

# CLIP 모델 관련 파라미터들
parameters [
  {
    key: "model_name"
    value: { string_value: "openai/clip-vit-large-patch14" }
  },
  {
    key: "max_sequence_length"
    value: { string_value: "77" }
  },
  {
    key: "embedding_dim"
    value: { string_value: "768" }
  }
]

# 버전 정책 (최신 버전 사용)
version_policy: { latest { num_versions: 1 } }

# 모델 워밍업
model_warmup [
  {
    name: "clip_warmup"
    batch_size: 1
    inputs [
      {
        key: "input_ids"
        value: {
          input_data_type: TYPE_INT64
          dims: [ 77 ]
          int64_data: [ 49406, 320, 2368, 5193, 320, 1320, 515, 1139, 7593, 1079, 49407 ]
          # 나머지는 0으로 패딩 (실제로는 77개 모두 필요)
        }
      }
    ]
  }
]