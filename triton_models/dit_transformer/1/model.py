#!/usr/bin/env python3
"""
DIT Transformer for Triton Inference Server

FLUX pipelineì˜ transformer í˜¸ì¶œ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
DIT (Diffusion Transformer)ë¥¼ ì‚¬ìš©í•œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ GPU ëª¨ë¸ì…ë‹ˆë‹¤.

Based on flux_pipeline.py lines 944-954 (denoising loopì˜ transformer í˜¸ì¶œ)
"""

import os
import sys
import json
import numpy as np
from typing import Dict, List, Optional

# Triton Python Backend
try:
    import triton_python_backend_utils as pb_utils
    TRITON_AVAILABLE = True
except ImportError:
    TRITON_AVAILABLE = False
    print("âš ï¸ Triton Python Backend not available - running in test mode")

# DLPack ì§€ì› í™•ì¸ (í—Œì¥ ìš”êµ¬ì‚¬í•­: ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜)
try:
    import torch.utils.dlpack
    DLPACK_AVAILABLE = True
except ImportError:
    DLPACK_AVAILABLE = False

# í•„ìˆ˜ ì˜ì¡´ì„±
try:
    import torch
    from diffusers import FluxTransformer2DModel, BitsAndBytesConfig as DiffusersBitsAndBytesConfig
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½: {e}")
    sys.exit(1)


class TritonPythonModel:
    """DIT Transformer Triton Model"""

    def initialize(self, args: Dict) -> None:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”„ DIT Transformer ì´ˆê¸°í™” ì¤‘...")

        # ëª¨ë¸ ì„¤ì • ë¡œë“œ
        self.model_config = json.loads(args['model_config'])

        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.model_name = self._get_parameter("model_name", "black-forest-labs/FLUX.1-schnell")
        self.in_channels = int(self._get_parameter("in_channels", "64"))
        self.num_inference_steps = int(self._get_parameter("num_inference_steps", "4"))
        self.use_fp8 = self._get_parameter("use_fp8", "false").lower() == "true"

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # DIT Transformer ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ DIT Transformer ë¡œë“œ ì¤‘: {self.model_name} (FP8: {self.use_fp8})")

        # FP8 quantization ì„¤ì • (flux_inference.py ë°©ì‹)
        if self.use_fp8:
            print("ğŸ”§ 8bit quantization ì‚¬ìš©")
            quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)
            self.transformer = FluxTransformer2DModel.from_pretrained(
                self.model_name,
                subfolder="transformer",
                quantization_config=quant_config,
                torch_dtype=torch.float16
            )
        else:
            print("ğŸ”§ FP16 ì‚¬ìš©")
            self.transformer = FluxTransformer2DModel.from_pretrained(
                self.model_name,
                subfolder="transformer",
                torch_dtype=torch.float16
            )
            self.transformer.to(self.device)
        self.transformer.eval()

        # joint_attention_kwargs ì´ˆê¸°í™”
        self.joint_attention_kwargs = {}

        # guidance_embeds ì„¤ì • í™•ì¸ (flux_pipeline.py ë¡œì§ ê¸°ë°˜)
        self.guidance_embeds = getattr(self.transformer.config, 'guidance_embeds', False)
        print(f"ğŸ“‹ guidance_embeds ì„¤ì •: {self.guidance_embeds}")

        print(f"âœ… DIT Transformer ë¡œë“œ ì™„ë£Œ: {self.model_name}")

    def _get_parameter(self, key: str, default: str = "") -> str:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        for param in self.model_config.get('parameters', []):
            if param['key'] == key:
                return param['value']['string_value']
        return default

    def execute(self, requests: List) -> List:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        responses = []

        for request in requests:
            try:
                response = self._process_request(request)
                responses.append(response)
            except Exception as e:
                error_msg = f"DIT Transformer ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                print(f"âŒ {error_msg}")

                # ì—ëŸ¬ ì‘ë‹µ ìƒì„±
                error_response = pb_utils.InferenceResponse(
                    error=pb_utils.TritonError(error_msg)
                ) if TRITON_AVAILABLE else None
                responses.append(error_response)

        return responses

    def _process_request(self, request):
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
        if TRITON_AVAILABLE:
            # Triton ëª¨ë“œ: ì‹¤ì œ ë°ì´í„°
            hidden_states = self._get_input_tensor(request, "hidden_states")
            timestep = self._get_input_tensor(request, "timestep")
            guidance = self._get_input_tensor(request, "guidance")
            pooled_projections = self._get_input_tensor(request, "pooled_projections")
            encoder_hidden_states = self._get_input_tensor(request, "encoder_hidden_states")
            txt_ids = self._get_input_tensor(request, "txt_ids")
            img_ids = self._get_input_tensor(request, "img_ids")
        else:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë”ë¯¸ ë°ì´í„°
            batch_size = 1
            num_patches = 2304  # ì‹¤ì œ FLUXì—ì„œ ê´€ì°°ëœ íŒ¨ì¹˜ ìˆ˜
            hidden_states = torch.randn((batch_size, num_patches, 64), dtype=torch.bfloat16, device=self.device)
            timestep = torch.tensor([0.5], dtype=torch.bfloat16, device=self.device)
            # guidance ì²˜ë¦¬ (flux_pipeline.py ë¡œì§ ê¸°ë°˜)
            guidance_scale = 0.0
            if hasattr(self, 'guidance_embeds') and self.guidance_embeds:
                guidance = torch.full([1], guidance_scale, device=self.device, dtype=torch.float32)
                guidance = guidance.expand(batch_size)
            else:
                guidance = None
            pooled_projections = torch.randn((batch_size, 768), dtype=torch.bfloat16, device=self.device)
            encoder_hidden_states = torch.randn((batch_size, 512, 4096), dtype=torch.bfloat16, device=self.device)
            txt_ids = torch.zeros((512, 3), dtype=torch.bfloat16, device=self.device)
            img_ids = torch.zeros((num_patches, 3), dtype=torch.bfloat16, device=self.device)

        # DIT Transformer ì¶”ë¡  ìˆ˜í–‰ (flux_pipeline.py ë¡œì§ ê¸°ë°˜)
        noise_pred = self._predict_noise(
            hidden_states, timestep, guidance, pooled_projections,
            encoder_hidden_states, txt_ids, img_ids
        )

        # ì‘ë‹µ ìƒì„±
        if TRITON_AVAILABLE:
            output_tensor = self._torch_to_tensor(noise_pred, "noise_pred")
            return pb_utils.InferenceResponse([output_tensor])
        else:
            return noise_pred

    def _get_input_tensor(self, request, name: str) -> torch.Tensor:
        """ì…ë ¥ í…ì„œ ì¶”ì¶œ ë° ë³€í™˜"""
        triton_tensor = pb_utils.get_input_tensor_by_name(request, name)
        return self._tensor_to_torch(triton_tensor)

    def _tensor_to_torch(self, triton_tensor) -> torch.Tensor:
        """Triton í…ì„œë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE and hasattr(triton_tensor, 'to_dlpack'):
            # DLPack ì‚¬ìš© (ë©”ëª¨ë¦¬ ë³µì‚¬ ìµœì†Œí™”)
            return torch.utils.dlpack.from_dlpack(triton_tensor.to_dlpack())
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = triton_tensor.as_numpy()
            tensor = torch.from_numpy(numpy_array).to(self.device)
            # bf16ìœ¼ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ìµœì í™”)
            return tensor.to(dtype=torch.bfloat16)

    def _torch_to_tensor(self, torch_tensor: torch.Tensor, name: str):
        """PyTorch í…ì„œë¥¼ Triton í…ì„œë¡œ ë³€í™˜"""
        if DLPACK_AVAILABLE:
            # DLPack ì‚¬ìš©
            return pb_utils.Tensor.from_dlpack(name, torch.utils.dlpack.to_dlpack(torch_tensor))
        else:
            # Fallback: numpyë¥¼ í†µí•œ ë³€í™˜
            numpy_array = torch_tensor.detach().cpu().numpy()
            return pb_utils.Tensor(name, numpy_array)

    def _predict_noise(
        self,
        hidden_states: torch.Tensor,
        timestep: torch.Tensor,
        guidance: torch.Tensor,
        pooled_projections: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        txt_ids: torch.Tensor,
        img_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        DIT Transformerë¥¼ ì‚¬ìš©í•œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡

        flux_pipeline.pyì˜ transformer í˜¸ì¶œ ë¡œì§ ê¸°ë°˜ (lines 944-954)

        Args:
            hidden_states: (batch_size, num_patches, 64) - í˜„ì¬ latent states
            timestep: (batch_size,) - ì •ê·œí™”ëœ timestep (0-1)
            guidance: (batch_size,) - guidance scale
            pooled_projections: (batch_size, 768) - CLIP pooled embeddings
            encoder_hidden_states: (batch_size, 512, 4096) - T5 sequence embeddings
            txt_ids: (batch_size, 512, 3) - í…ìŠ¤íŠ¸ ìœ„ì¹˜ ID
            img_ids: (batch_size, num_patches, 3) - ì´ë¯¸ì§€ ìœ„ì¹˜ ID

        Returns:
            noise_pred: (batch_size, num_patches, 64) - ì˜ˆì¸¡ëœ ë…¸ì´ì¦ˆ
        """
        # ì…ë ¥ í…ì„œë“¤ì„ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì™€ dtypeìœ¼ë¡œ ë³€í™˜
        hidden_states = hidden_states.to(device=self.device, dtype=self.transformer.dtype)
        timestep = timestep.to(device=self.device, dtype=self.transformer.dtype)
        # guidanceëŠ” Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬
        if guidance is not None:
            guidance = guidance.to(device=self.device, dtype=self.transformer.dtype)
        pooled_projections = pooled_projections.to(device=self.device, dtype=self.transformer.dtype)
        encoder_hidden_states = encoder_hidden_states.to(device=self.device, dtype=self.transformer.dtype)
        txt_ids = txt_ids.to(device=self.device, dtype=self.transformer.dtype)
        img_ids = img_ids.to(device=self.device, dtype=self.transformer.dtype)

        # ë°°ì¹˜ í¬ê¸° í™•ì¥ (flux_pipeline.py line 941 ê¸°ë°˜)
        batch_size = hidden_states.shape[0]
        if timestep.dim() == 1 and timestep.shape[0] == 1:
            timestep = timestep.expand(batch_size)
        # guidanceê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ í™•ì¥
        if guidance is not None and guidance.dim() == 1 and guidance.shape[0] == 1:
            guidance = guidance.expand(batch_size)

        print(f"ğŸ“Š DIT ì…ë ¥ í˜•íƒœ:")
        print(f"  hidden_states: {hidden_states.shape}")  # [batch_size, num_patches, 64]
        print(f"  timestep: {timestep.shape}")  # [batch_size]
        print(f"  guidance: {guidance.shape if guidance is not None else 'None'}")  # [batch_size] or None
        print(f"  pooled_projections: {pooled_projections.shape}")  # [batch_size, 768]
        print(f"  encoder_hidden_states: {encoder_hidden_states.shape}")  # [batch_size, 512, 4096]

        with torch.no_grad():
            # DIT Transformer ì‹¤í–‰ (flux_pipeline.py lines 944-954 ê¸°ë°˜)
            # guidanceëŠ” Noneì´ë¼ë„ í•­ìƒ ì „ë‹¬ (transformer.config.guidance_embedsì— ë”°ë¼ ì²˜ë¦¬ë¨)
            transformer_output = self.transformer(
                hidden_states=hidden_states,
                timestep=timestep / 1000,  # timestep ì •ê·œí™”
                guidance=guidance,  # Noneì´ë¼ë„ í•­ìƒ ì „ë‹¬
                pooled_projections=pooled_projections,
                encoder_hidden_states=encoder_hidden_states,
                txt_ids=txt_ids,
                img_ids=img_ids,
                joint_attention_kwargs=self.joint_attention_kwargs,
                return_dict=False,
            )

            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì¶œë ¥)
            noise_pred = transformer_output[0]

            print(f"ğŸ“Š DIT ì¶œë ¥ í˜•íƒœ: {noise_pred.shape}")  # [batch_size, num_patches, 64]

            return noise_pred

    def finalize(self) -> None:
        """ëª¨ë¸ ì •ë¦¬"""
        print("ğŸ”„ DIT Transformer ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'transformer'):
            del self.transformer

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("âœ… DIT Transformer ì •ë¦¬ ì™„ë£Œ")


def test_dit_transformer():
    """ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (í—Œì¥ ìš”êµ¬ì‚¬í•­)"""
    print("ğŸš€ DIT Transformer í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
    os.environ["TEST_MODE"] = "true"

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TritonPythonModel()
    args = {
        'model_config': json.dumps({
            'parameters': [
                {'key': 'model_name', 'value': {'string_value': 'black-forest-labs/FLUX.1-schnell'}},
                {'key': 'in_channels', 'value': {'string_value': '64'}},
                {'key': 'num_inference_steps', 'value': {'string_value': '4'}},
                {'key': 'use_fp8', 'value': {'string_value': 'true'}}
            ]
        })
    }

    try:
        model.initialize(args)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (4íšŒ ë°˜ë³µ í˜¸ì¶œì„ ìœ„í•œ ì„¤ê³„)
        batch_size = 1
        num_patches = 2304  # ì‹¤ì œ FLUXì—ì„œ ê´€ì°°ëœ íŒ¨ì¹˜ ìˆ˜

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì…ë ¥ í˜•íƒœ:")
        print(f"  batch_size: {batch_size}")
        print(f"  num_patches: {num_patches}")

        # 4íšŒ ë°˜ë³µ ì‹œë®¬ë ˆì´ì…˜ (denoising steps)
        for step in range(4):
            print(f"\nğŸ”„ Step {step + 1}/4")

            # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
            hidden_states = torch.randn((batch_size, num_patches, 64), dtype=torch.bfloat16)
            timestep = torch.tensor([1.0 - step * 0.25], dtype=torch.bfloat16)  # 1.0 -> 0.0

            # guidance ì²˜ë¦¬ (transformer.config.guidance_embedsì— ë”°ë¼)
            guidance_scale = 0.0
            if hasattr(model, 'guidance_embeds') and model.guidance_embeds:
                guidance = torch.full([1], guidance_scale, device=model.device, dtype=torch.float32)
                guidance = guidance.expand(batch_size)
            else:
                guidance = None
            pooled_projections = torch.randn((batch_size, 768), dtype=torch.bfloat16)
            encoder_hidden_states = torch.randn((batch_size, 512, 4096), dtype=torch.bfloat16)
            txt_ids = torch.zeros((512, 3), dtype=torch.bfloat16)
            img_ids = torch.zeros((num_patches, 3), dtype=torch.bfloat16)

            # ì¶”ë¡  ì‹¤í–‰
            noise_pred = model._predict_noise(
                hidden_states, timestep, guidance, pooled_projections,
                encoder_hidden_states, txt_ids, img_ids
            )

            # ê²°ê³¼ ê²€ì¦
            expected_shape = (batch_size, num_patches, 64)
            if noise_pred.shape == expected_shape:
                print(f"âœ… Step {step + 1} ì¶œë ¥ í˜•íƒœ ê²€ì¦ í†µê³¼: {noise_pred.shape}")
            else:
                print(f"âŒ Step {step + 1} ì¶œë ¥ í˜•íƒœ ì˜¤ë¥˜: {noise_pred.shape}, ì˜ˆìƒ: {expected_shape}")
                return False

            # ê°’ ë²”ìœ„ ê²€ì¦
            min_val, max_val = noise_pred.min().item(), noise_pred.max().item()
            print(f"ğŸ“Š Step {step + 1} ì¶œë ¥ ê°’ ë²”ìœ„: {min_val:.4f} ~ {max_val:.4f}")

        model.finalize()
        print("\nğŸ‰ DIT Transformer 4íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    success = test_dit_transformer()
    sys.exit(0 if success else 1)