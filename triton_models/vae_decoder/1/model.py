#!/usr/bin/env python3
"""
VAE Decoder for Triton Inference Server

FLUX pipelineì˜ VAE decoder ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
latent spaceì—ì„œ RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” GPU ëª¨ë¸ì…ë‹ˆë‹¤.

Based on flux_pipeline.py lines 1004-1007, 529-542
"""

import os
import sys
import json
import numpy as np
from typing import Dict, List, Optional, Tuple

# ===================================================================
# ê³µí†µ í•¨ìˆ˜ë“¤ (common_functions_template.pyì—ì„œ ë³µì‚¬)
# ===================================================================

def check_triton_availability():
    """Triton Python Backend ê°€ìš©ì„± ì²´í¬ ë° Mock ì„¤ì •"""
    try:
        import triton_python_backend_utils as pb_utils
        return True, pb_utils
    except ImportError:
        import logging
        logging.warning("Triton backend not available - running in test mode")

        class MockPbUtils:
            class Tensor:
                def __init__(self, name, data): pass
            class InferenceRequest: pass
            class InferenceResponse:
                def __init__(self, output_tensors=None, error=None): pass
            class TritonError:
                def __init__(self, message): pass

        return False, MockPbUtils()

def initialize_model_base(args, model_name: str, required_modules):
    """ëª¨ë“  ëª¨ë¸ì˜ ê³µí†µ ì´ˆê¸°í™” ë¡œì§"""
    import json
    import logging
    logging.basicConfig(level=logging.INFO, format=f'[{model_name}] %(message)s')

    # Triton ê°€ìš©ì„± ì²´í¬
    triton_available, pb_utils = check_triton_availability()

    # ëª¨ë¸ config íŒŒì‹±
    model_config = json.loads(args['model_config'])
    params = {}
    for param in model_config.get('parameters', []):
        params[param['key']] = param['value']['string_value']

    return {
        'triton_available': triton_available,
        'pb_utils': pb_utils,
        'model_config': model_config,
        'params': params
    }

def handle_model_error(pb_utils, triton_available: bool, error: Exception, context: str = ""):
    """ëª¨ë¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
    import logging
    error_msg = f"{context} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(error)}"
    logging.error(error_msg)

    if triton_available:
        return pb_utils.InferenceResponse(
            output_tensors=[],
            error=pb_utils.TritonError(error_msg)
        )
    return None

# ===================================================================

# í•„ìˆ˜ ì˜ì¡´ì„±
try:
    import torch
    from diffusers import AutoencoderKL
    from diffusers.image_processor import VaeImageProcessor
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½: {e}")
    sys.exit(1)


class TritonPythonModel:
    """VAE Decoder Triton Model"""

    def initialize(self, args: Dict) -> None:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        # ê³µí†µ ì´ˆê¸°í™” ë¡œì§ ì‚¬ìš©
        init_result = initialize_model_base(args, "VAE_Decoder", ["torch", "diffusers"])

        # ì´ˆê¸°í™” ê²°ê³¼ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
        self.triton_available = init_result['triton_available']
        self.pb_utils = init_result['pb_utils']
        self.model_config = init_result['model_config']
        params = init_result['params']

        # VAE íŠ¹í™” íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.model_name = params.get('model_name', 'black-forest-labs/FLUX.1-schnell')
        self.vae_scale_factor = float(params.get('vae_scale_factor', '8'))
        self.scaling_factor = float(params.get('scaling_factor', '0.3611'))
        self.shift_factor = float(params.get('shift_factor', '0.1159'))

        # GPU ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“ Device: {self.device}")

        # VAE ëª¨ë¸ ë¡œë“œ
        try:
            self.vae = AutoencoderKL.from_pretrained(
                self.model_name,
                subfolder="vae",
                torch_dtype=torch.bfloat16  # ë©”ëª¨ë¦¬ ìµœì í™”
            ).to(self.device)

            # VAE configì—ì„œ ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì—…ë°ì´íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(self.vae.config, 'scaling_factor'):
                self.scaling_factor = self.vae.config.scaling_factor
            if hasattr(self.vae.config, 'shift_factor'):
                self.shift_factor = self.vae.config.shift_factor

            print(f"âœ… VAE ë¡œë“œ ì™„ë£Œ: scaling_factor={self.scaling_factor}, shift_factor={self.shift_factor}")

        except Exception as e:
            print(f"âŒ VAE ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

        # ì´ë¯¸ì§€ í›„ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.image_processor = VaeImageProcessor()

        # VAEë¥¼ evaluation ëª¨ë“œë¡œ ì„¤ì •
        self.vae.eval()

        print("âœ… VAE Decoder ì´ˆê¸°í™” ì™„ë£Œ")


    def _unpack_latents(self, latents: torch.Tensor, height: int, width: int) -> torch.Tensor:
        """
        Latent unpacking êµ¬í˜„ (flux_pipeline.py lines 529-542)

        Args:
            latents: íŒ¨í‚¹ëœ latent í…ì„œ (batch_size, num_patches, 64)
            height: ëª©í‘œ ì´ë¯¸ì§€ ë†’ì´
            width: ëª©í‘œ ì´ë¯¸ì§€ ë„ˆë¹„

        Returns:
            ì–¸íŒ¨í‚¹ëœ latent í…ì„œ (batch_size, 16, latent_height, latent_width)
        """
        batch_size, num_patches, channels = latents.shape

        # VAEëŠ” 8x ì••ì¶•ì„ ì ìš©í•˜ì§€ë§Œ íŒ¨í‚¹ìœ¼ë¡œ ì¸í•´ latent ë†’ì´/ë„ˆë¹„ê°€ 2ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨
        height = 2 * (int(height) // (int(self.vae_scale_factor) * 2))
        width = 2 * (int(width) // (int(self.vae_scale_factor) * 2))

        # íŒ¨í‚¹ í•´ì œ: (batch_size, num_patches, 64) -> (batch_size, 16, height, width)
        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)
        latents = latents.permute(0, 3, 1, 4, 2, 5)
        latents = latents.reshape(batch_size, channels // (2 * 2), height, width)

        return latents

    def _tensor_from_dlpack_or_numpy(self, pb_tensor) -> torch.Tensor:
        """DLPack ë˜ëŠ” numpyë¥¼ í†µí•œ í…ì„œ ë³€í™˜ (ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜)"""
        try:
            import torch.utils.dlpack
            dlpack_available = True
        except ImportError:
            dlpack_available = False

        if dlpack_available and hasattr(pb_tensor, 'to_dlpack'):
            try:
                return torch.utils.dlpack.from_dlpack(pb_tensor.to_dlpack())
            except Exception:
                pass  # fallback to numpy

        # numpy fallback
        numpy_array = pb_tensor.as_numpy()
        return torch.from_numpy(numpy_array)

    def _tensor_to_pb_tensor(self, tensor: torch.Tensor, name: str):
        """torch.Tensorë¥¼ pb_utils.Tensorë¡œ ë³€í™˜"""
        try:
            import torch.utils.dlpack
            dlpack_available = True
        except ImportError:
            dlpack_available = False

        if dlpack_available and tensor.is_cuda:
            try:
                dlpack_tensor = torch.utils.dlpack.to_dlpack(tensor)
                return self.pb_utils.Tensor.from_dlpack(name, dlpack_tensor)
            except Exception:
                pass  # fallback to numpy

        # numpy fallback
        numpy_array = tensor.cpu().numpy()
        return self.pb_utils.Tensor(name, numpy_array)

    def execute(self, requests: List) -> List:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        responses = []

        for request in requests:
            try:
                # ì…ë ¥ í…ì„œ ì¶”ì¶œ
                latents_pb = self.pb_utils.get_input_tensor_by_name(request, "latents")
                height_pb = self.pb_utils.get_input_tensor_by_name(request, "height")
                width_pb = self.pb_utils.get_input_tensor_by_name(request, "width")

                # í…ì„œ ë³€í™˜
                latents = self._tensor_from_dlpack_or_numpy(latents_pb).to(
                    device=self.device, dtype=torch.bfloat16
                )
                height = int(height_pb.as_numpy()[0])
                width = int(width_pb.as_numpy()[0])

                # VAE ë””ì½”ë”© ìˆ˜í–‰
                with torch.no_grad():
                    # 1. Latent unpacking (flux_pipeline.py lines 529-542)
                    unpacked_latents = self._unpack_latents(latents, height, width)

                    # 2. Scaling ì ìš© (flux_pipeline.py line 1005)
                    scaled_latents = (unpacked_latents / self.scaling_factor) + self.shift_factor

                    # 3. VAE decode (flux_pipeline.py line 1006)
                    decoded_images = self.vae.decode(scaled_latents, return_dict=False)[0]

                    # 4. í›„ì²˜ë¦¬ (flux_pipeline.py line 1007)
                    # VaeImageProcessor.postprocessë¥¼ ê°„ì†Œí™”í•œ ë²„ì „
                    # RGB ê°’ì„ [0, 1] ë²”ìœ„ë¡œ í´ë¨í•‘
                    images = torch.clamp((decoded_images + 1.0) / 2.0, 0.0, 1.0)

                # ì¶œë ¥ í…ì„œ ë³€í™˜
                output_tensor = self._tensor_to_pb_tensor(images, "images")
                inference_response = self.pb_utils.InferenceResponse([output_tensor])
                responses.append(inference_response)

            except Exception as e:
                error_response = handle_model_error(
                    self.pb_utils,
                    self.triton_available,
                    e,
                    "VAE Decoder"
                )
                responses.append(error_response)

        return responses

    def finalize(self) -> None:
        """ëª¨ë¸ ì •ë¦¬"""
        print("ğŸ”„ VAE Decoder ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'vae'):
            del self.vae
        torch.cuda.empty_cache()
        print("âœ… VAE Decoder ì •ë¦¬ ì™„ë£Œ")


# ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (í—Œì¥ ìš”êµ¬ì‚¬í•­)
def test_vae_decoder():
    """VAE decoder í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª VAE Decoder í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # DLPack ì‚¬ìš© ì—¬ë¶€ ê²°ì • (í…ŒìŠ¤íŠ¸ ì‹œ ë¶„ê¸°ì²˜ë¦¬)
    try:
        import torch.utils.dlpack
        dlpack_available = True
    except ImportError:
        dlpack_available = False
    use_dlpack = dlpack_available and torch.cuda.is_available()
    print(f"ğŸ“Š DLPack ì‚¬ìš©: {use_dlpack}")

    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì •
    model_config = {
        "parameters": [
            {"key": "model_name", "value": {"string_value": "black-forest-labs/FLUX.1-schnell"}},
            {"key": "vae_scale_factor", "value": {"string_value": "8"}},
            {"key": "scaling_factor", "value": {"string_value": "0.3611"}},
            {"key": "shift_factor", "value": {"string_value": "0.1159"}}
        ]
    }

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TritonPythonModel()
    args = {"model_config": json.dumps(model_config)}

    try:
        model.initialize(args)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        batch_size = 1
        height, width = 1024, 1024
        latent_height = height // 8  # VAE scale factor
        latent_width = width // 8
        num_patches = (latent_height // 2) * (latent_width // 2)  # íŒ¨í‚¹ ê³ ë ¤

        # íŒ¨í‚¹ëœ latent í…ì„œ ìƒì„± (batch_size, num_patches, 64)
        test_latents = torch.randn(batch_size, num_patches, 64, dtype=torch.bfloat16)
        test_height = torch.tensor([height], dtype=torch.int32)
        test_width = torch.tensor([width], dtype=torch.int32)

        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì…ë ¥ shape: latents={test_latents.shape}, height={height}, width={width}")

        # Mock request ìƒì„± (triton_available=Falseì¼ ë•Œ)
        triton_available, _ = check_triton_availability()
        if not triton_available:
            class MockTensor:
                def __init__(self, data):
                    self.data = data

                def as_numpy(self):
                    if isinstance(self.data, torch.Tensor):
                        return self.data.numpy()
                    return self.data

            class MockRequest:
                def __init__(self, latents, height, width):
                    self.tensors = {
                        "latents": MockTensor(latents),
                        "height": MockTensor(height),
                        "width": MockTensor(width)
                    }

            # pb_utils.get_input_tensor_by_name ëª¨í‚¹
            import types
            mock_pb_utils = types.ModuleType('pb_utils')

            def mock_get_input_tensor_by_name(request, name):
                return request.tensors[name]

            mock_pb_utils.get_input_tensor_by_name = mock_get_input_tensor_by_name
            sys.modules['triton_python_backend_utils'] = mock_pb_utils

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            mock_request = MockRequest(test_latents, test_height, test_width)

            # ì§ì ‘ VAE ë””ì½”ë”© í…ŒìŠ¤íŠ¸
            with torch.no_grad():
                unpacked = model._unpack_latents(test_latents.to(model.device), height, width)
                print(f"ğŸ“ ì–¸íŒ¨í‚¹ëœ latent shape: {unpacked.shape}")

                scaled = (unpacked / model.scaling_factor) + model.shift_factor
                decoded = model.vae.decode(scaled, return_dict=False)[0]
                images = torch.clamp((decoded + 1.0) / 2.0, 0.0, 1.0)

                print(f"ğŸ“ ìµœì¢… ì´ë¯¸ì§€ shape: {images.shape}")
                print(f"ğŸ“Š ì´ë¯¸ì§€ ê°’ ë²”ìœ„: [{images.min().item():.3f}, {images.max().item():.3f}]")

        print("âœ… VAE Decoder í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

    finally:
        model.finalize()


if __name__ == "__main__":
    test_vae_decoder()