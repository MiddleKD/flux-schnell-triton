# Quickstart: FLUX to Triton Conversion

## ê°œìš”
ì´ ê°€ì´ë“œëŠ” FLUX íŒŒì´í”„ë¼ì¸ì„ Triton ì¶”ë¡  ì„œë²„ë¡œ ë³€í™˜í•œ ì‹œìŠ¤í…œì„ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì „ì œ ì¡°ê±´
- Python 3.8+
- NVIDIA GPU (CUDA ì§€ì›)
- Triton Inference Server
- í•„ìš”í•œ Python íŒ¨í‚¤ì§€: torch, transformers, diffusers

## 1. í™˜ê²½ ì„¤ì •

### ê°€ìƒí™˜ê²½ ìƒì„± (uv ì‚¬ìš©)
```bash
# uv ì„¤ì¹˜ (í—Œì¥ ìš”êµ¬ì‚¬í•­)
curl -LsSf https://astral.sh/uv/install.sh | sh

# í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
uv init
uv add torch transformers diffusers tritonclient[all]
```

### Triton ì„œë²„ ì‹œì‘
```bash
# Dockerë¥¼ ì‚¬ìš©í•œ Triton ì„œë²„ ì‹œì‘
docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 \
  -v$(pwd)/triton_models:/models \
  nvcr.io/nvidia/tritonserver:23.10-py3 \
  tritonserver --model-repository=/models
```

## 2. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì´ë¯¸ì§€ ìƒì„±
**ëª©í‘œ**: ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜ ê²€ì¦

```python
# test_single_prompt.py
import requests
import json
import numpy as np
from PIL import Image

def test_single_prompt():
    """flux_pipeline.pyì˜ ê¸°ë³¸ ì˜ˆì œë¥¼ ì¬í˜„"""

    # BLS ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ
    url = "http://localhost:8000/v2/models/bls/infer"

    payload = {
        "inputs": [
            {
                "name": "prompt",
                "shape": [1],
                "datatype": "BYTES",
                "data": ["A cat holding a sign that says hello world"]
            },
            {
                "name": "num_inference_steps",
                "shape": [1],
                "datatype": "INT32",
                "data": [4]
            },
            {
                "name": "guidance_scale",
                "shape": [1],
                "datatype": "FP32",
                "data": [0.0]
            }
        ]
    }

    response = requests.post(url, json=payload)
    assert response.status_code == 200

    result = response.json()
    image_data = np.array(result["outputs"][0]["data"])

    # ì´ë¯¸ì§€ í˜•íƒœ ë³µì› (1, 3, 1024, 1024)
    image_shape = result["outputs"][0]["shape"]
    image_array = image_data.reshape(image_shape)

    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ ë° ì €ì¥
    image_rgb = (image_array[0].transpose(1, 2, 0) * 255).astype(np.uint8)
    image = Image.fromarray(image_rgb)
    image.save("test_output.png")

    print("âœ… ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

if __name__ == "__main__":
    test_single_prompt()
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
**ëª©í‘œ**: ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ ê²€ì¦

```python
# test_batch_processing.py
def test_batch_processing():
    """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""

    url = "http://localhost:8000/v2/models/bls/infer"

    payload = {
        "inputs": [
            {
                "name": "prompt",
                "shape": [2],
                "datatype": "BYTES",
                "data": [
                    "A beautiful landscape with mountains",
                    "A futuristic city with flying cars"
                ]
            },
            {
                "name": "num_inference_steps",
                "shape": [1],
                "datatype": "INT32",
                "data": [4]
            }
        ]
    }

    response = requests.post(url, json=payload)
    assert response.status_code == 200

    result = response.json()

    # ë°°ì¹˜ í¬ê¸° ê²€ì¦
    image_shape = result["outputs"][0]["shape"]
    assert image_shape[0] == 2, f"Expected batch_size=2, got {image_shape[0]}"

    print("âœ… ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

if __name__ == "__main__":
    test_batch_processing()
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸
**ëª©í‘œ**: ê° Triton ëª¨ë¸ì˜ ë…ë¦½ì  ë™ì‘ ê²€ì¦

```python
# test_individual_models.py
def test_clip_encoder():
    """CLIP ì¸ì½”ë” ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    from transformers import CLIPTokenizer

    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
    text = "A cat holding a sign"

    # í† í°í™”
    tokens = tokenizer(text, padding="max_length", max_length=77,
                      truncation=True, return_tensors="pt")

    url = "http://localhost:8001/v2/models/clip_encoder/infer"
    payload = {
        "inputs": [
            {
                "name": "input_ids",
                "shape": list(tokens.input_ids.shape),
                "datatype": "INT64",
                "data": tokens.input_ids.flatten().tolist()
            }
        ]
    }

    response = requests.post(url, json=payload)
    assert response.status_code == 200

    result = response.json()
    embeddings_shape = result["outputs"][0]["shape"]
    assert embeddings_shape == [1, 768], f"Expected [1, 768], got {embeddings_shape}"

    print("âœ… CLIP ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

def test_t5_encoder():
    """T5 ì¸ì½”ë” ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    from transformers import T5TokenizerFast

    tokenizer = T5TokenizerFast.from_pretrained("google/t5-v1_1-xxl")
    text = "A cat holding a sign"

    tokens = tokenizer(text, padding="max_length", max_length=512,
                      truncation=True, return_tensors="pt")

    url = "http://localhost:8001/v2/models/t5_encoder/infer"
    payload = {
        "inputs": [
            {
                "name": "input_ids",
                "shape": list(tokens.input_ids.shape),
                "datatype": "INT64",
                "data": tokens.input_ids.flatten().tolist()
            }
        ]
    }

    response = requests.post(url, json=payload)
    assert response.status_code == 200

    result = response.json()
    # sequence_embeds output ê²€ì¦
    seq_embeds = [out for out in result["outputs"] if out["name"] == "sequence_embeds"][0]
    assert seq_embeds["shape"][2] == 4096, "T5 embedding dimension should be 4096"

    print("âœ… T5 ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

if __name__ == "__main__":
    test_clip_encoder()
    test_t5_encoder()
```

## 3. ì„±ëŠ¥ ê²€ì¦

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```python
# test_memory_usage.py
import psutil
import GPUtil

def monitor_memory_usage():
    """DLPack ìµœì í™” íš¨ê³¼ ê²€ì¦"""

    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    gpu = GPUtil.getGPUs()[0]
    initial_gpu_memory = gpu.memoryUsed

    # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    process = psutil.Process()
    initial_cpu_memory = process.memory_info().rss / 1024 / 1024  # MB

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_single_prompt()

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬ì¸¡ì •
    gpu = GPUtil.getGPUs()[0]
    final_gpu_memory = gpu.memoryUsed
    final_cpu_memory = process.memory_info().rss / 1024 / 1024

    print(f"GPU ë©”ëª¨ë¦¬ ì¦ê°€: {final_gpu_memory - initial_gpu_memory}MB")
    print(f"CPU ë©”ëª¨ë¦¬ ì¦ê°€: {final_cpu_memory - initial_cpu_memory}MB")

    # DLPack ì‚¬ìš©ìœ¼ë¡œ CPU ë©”ëª¨ë¦¬ ì¦ê°€ê°€ ìµœì†Œí™”ë˜ì–´ì•¼ í•¨
    assert final_cpu_memory - initial_cpu_memory < 100, "CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ í¼"

    print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ê²€ì¦ ì„±ê³µ!")
```

## 4. ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦

### ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸
```python
# test_graceful_degradation.py
def test_missing_dependencies():
    """DLPack ë¯¸ì§€ì› í™˜ê²½ì—ì„œì˜ ë™ì‘ ê²€ì¦"""

    # DLPack ëª¨ë“ˆ ì„ì‹œ ì œê±° ì‹œë®¬ë ˆì´ì…˜
    import sys
    if 'dlpack' in sys.modules:
        del sys.modules['dlpack']

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (fallback ëª¨ë“œë¡œ ë™ì‘í•´ì•¼ í•¨)
    test_single_prompt()
    print("âœ… DLPack ë¯¸ì§€ì› í™˜ê²½ì—ì„œë„ ì •ìƒ ë™ì‘!")

def test_memory_limit():
    """GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ìƒí™© ì²˜ë¦¬"""

    # ë§¤ìš° í° ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ ë°œ
    url = "http://localhost:8000/v2/models/bls/infer"
    payload = {
        "inputs": [
            {
                "name": "prompt",
                "shape": [100],  # ê³¼ë„í•œ ë°°ì¹˜ í¬ê¸°
                "datatype": "BYTES",
                "data": ["test prompt"] * 100
            }
        ]
    }

    response = requests.post(url, json=payload)

    # ì—ëŸ¬ê°€ ë°œìƒí•˜ë”ë¼ë„ ì„œë²„ê°€ ì£½ì§€ ì•Šê³  ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
    if response.status_code != 200:
        error_msg = response.json().get("error", "")
        assert "memory" in error_msg.lower(), "ë©”ëª¨ë¦¬ ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë¶€ì¡±í•¨"
        print("âœ… ë©”ëª¨ë¦¬ ë¶€ì¡± ìƒí™© ì ì ˆíˆ ì²˜ë¦¬ë¨!")
    else:
        print("âš ï¸ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë„ ì²˜ë¦¬ ê°€ëŠ¥ (GPU ë©”ëª¨ë¦¬ ì¶©ë¶„)")
```

## 5. ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```python
# run_all_tests.py
def run_quickstart_tests():
    """ëª¨ë“  quickstart í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    print("ğŸš€ FLUX to Triton ë³€í™˜ ì‹œìŠ¤í…œ ê²€ì¦ ì‹œì‘")

    try:
        # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        test_single_prompt()
        test_batch_processing()

        # ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        test_clip_encoder()
        test_t5_encoder()

        # ì„±ëŠ¥ ê²€ì¦
        monitor_memory_usage()

        # ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦
        test_missing_dependencies()
        test_memory_limit()

        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œì´ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤.")

    except AssertionError as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        print(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

    return True

if __name__ == "__main__":
    success = run_quickstart_tests()
    exit(0 if success else 1)
```

## 6. ë””ë²„ê¹… ê°€ì´ë“œ

### ë¡œê·¸ í™•ì¸
```bash
# Triton ì„œë²„ ë¡œê·¸
docker logs <triton_container_id>

# ê°œë³„ ëª¨ë¸ ë¡œê·¸
curl http://localhost:8000/v2/models/bls/stats
```

### ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°
1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**: ë°°ì¹˜ í¬ê¸° ê°ì†Œ ë˜ëŠ” GPU ë©”ëª¨ë¦¬ ì •ë¦¬
2. **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ í™•ì¸
3. **í† í°í™” ì˜¤ë¥˜**: ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ë° íŠ¹ìˆ˜ë¬¸ì í™•ì¸

ì´ quickstart ê°€ì´ë“œë¥¼ í†µí•´ FLUX to Triton ë³€í™˜ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.